{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d052e9ea-f4f6-4a24-81d2-64c7a891a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import socks\n",
    "import socket\n",
    "\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "socket.socket = socks.socksocket\n",
    "\n",
    "def load_page(url):\n",
    "    try:\n",
    "        ua = UserAgent()\n",
    "        headers = {'User-Agent': ua.random}\n",
    "        page = requests.get(url, headers=headers)\n",
    "        time.sleep(random.uniform(0.5,1.5))  \n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        if soup is not None:\n",
    "            return soup\n",
    "        else:\n",
    "            time.sleep(60)\n",
    "    except requests.ConnectionError:\n",
    "        print(\"Ошибка: Нет интернет-соединения.\")\n",
    "        time.sleep(600)\n",
    "        return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Ошибка при загрузке страницы {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_match_info(match_url):\n",
    "    soup = load_page(match_url)\n",
    "    if soup is not None:\n",
    "        if soup.find('div', class_='header-content-secondary'):\n",
    "            datetime = soup.find('div', class_='header-content-secondary').find('time').get('datetime')\n",
    "        else:\n",
    "            time.sleep(10)\n",
    "            return None\n",
    "        try:\n",
    "            team_1 = soup.find('div', class_='team-results').find('section', class_='radiant').find('span', class_='team-text team-text-full').text\n",
    "        except:\n",
    "            team_1 = 'Radiant'\n",
    "        try:\n",
    "            team_2 = soup.find('div', class_='team-results').find('section', class_='dire').find('span', class_='team-text team-text-full').text\n",
    "        except:\n",
    "            team_2 = 'Dire'\n",
    "        try:\n",
    "            try:\n",
    "                result = soup.find('div', class_='match-result team radiant').find('span', class_=\"team-text team-text-full\").text.strip()\n",
    "            except:\n",
    "                result = soup.find('div', class_='match-result team dire').find('span', class_=\"team-text team-text-full\").text.strip()\n",
    "        except:\n",
    "            try:\n",
    "                result = soup.find('div', class_='match-result team radiant').text.strip().replace(' Victory', '')\n",
    "            except:\n",
    "                result = soup.find('div', class_='match-result team dire').text.strip().replace(' Victory', '')\n",
    "        heroes = []\n",
    "        hero_elements = soup.find_all('div', class_='image-container image-container-hero image-container-icon')\n",
    "        for hero_element in hero_elements:\n",
    "            hero_name = hero_element.find('a').find('img').get('title').strip()\n",
    "            heroes.append(hero_name)\n",
    "        try:\n",
    "            match_info = {\n",
    "            'DateTime': datetime,\n",
    "            'Team_Radiant': team_1,\n",
    "            'Team_Dire': team_2,\n",
    "            'Result': result,\n",
    "            'DateTime': datetime,\n",
    "            'Hero_Rad_1': heroes[0],\n",
    "            'Hero_Rad_2': heroes[1],\n",
    "            'Hero_Rad_3': heroes[2],\n",
    "            'Hero_Rad_4': heroes[3],\n",
    "            'Hero_Rad_5': heroes[4],\n",
    "            'Hero_Dire_1': heroes[5],\n",
    "            'Hero_Dire_2': heroes[6],\n",
    "            'Hero_Dire_3': heroes[7],\n",
    "            'Hero_Dire_4': heroes[8],\n",
    "            'Hero_Dire_5': heroes[9]\n",
    "        }\n",
    "            return match_info\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "start_urls = [\n",
    "             ]\n",
    "page_urls = start_urls\n",
    "for start_url in start_urls:\n",
    "    soup = load_page(start_url)\n",
    "    if soup is not None:\n",
    "        page = soup.find('span', class_='next')\n",
    "        try:\n",
    "            if 'https://www.dotabuff.com' + page.find('a')['href'] in page_urls:\n",
    "                continue\n",
    "            else:\n",
    "                page_urls.append('https://www.dotabuff.com' + page.find('a')['href'])\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "print(len(page_urls))\n",
    "\n",
    "match_urls = []\n",
    "for page in page_urls:\n",
    "    soup = load_page(page)\n",
    "    matchestd = soup.find_all('td', class_='cell-large')\n",
    "    for matches in matchestd:\n",
    "        matchesa = matches.find_all('a')\n",
    "        for match in matchesa:\n",
    "            if 'https://www.dotabuff.com' + match['href'] in match_urls:\n",
    "                continue\n",
    "            else:\n",
    "                match_urls.append('https://www.dotabuff.com' + match['href'])\n",
    "                \n",
    "print(len(match_urls))\n",
    "\n",
    "data = []\n",
    "for match_url in match_urls:\n",
    "    match_info = get_match_info(match_url)\n",
    "    if match_info != None:\n",
    "        data.append(match_info)\n",
    "        if len(data) % 100 == 0:\n",
    "            print(len(data))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Team_Radiant', 'Team_Dire', 'Hero_Rad_1', 'Hero_Rad_2', 'Hero_Rad_3', 'Hero_Rad_4', 'Hero_Rad_5', \\\n",
    "                                 'Hero_Dire_1', 'Hero_Dire_2', 'Hero_Dire_3', 'Hero_Dire_4', 'Hero_Dire_5', 'DateTime', 'Result'])\n",
    "\n",
    "desktop_path = str(Path.home() / \"Desktop\")\n",
    "\n",
    "df.to_csv(os.path.join(desktop_path, \"dota2dataset.csv\"), index=False)\n",
    "\n",
    "\n",
    "print(df.head(), df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2eb0e2-2527-4b02-9b5c-73cee0c2957a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
